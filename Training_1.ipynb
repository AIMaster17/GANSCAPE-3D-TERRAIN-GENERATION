{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c81a741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6f43ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 1. Device, Seeds, and Global Settings\n",
    "# ===================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56ea046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 2. Data Preprocessing\n",
    "# ===================================================\n",
    "class JointTransform:\n",
    "    def __init__(self, size=(512, 512)):\n",
    "        self.size = size\n",
    "    def __call__(self, seg, height):\n",
    "        seg = seg.resize(self.size, Image.BICUBIC)\n",
    "        height = height.resize(self.size, Image.BICUBIC)\n",
    "        if random.random() > 0.5:\n",
    "            seg = seg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            height = height.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if random.random() > 0.5:\n",
    "            seg = seg.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            height = height.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return seg, height\n",
    "\n",
    "transform_seg = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "def refined_height_transform(x):\n",
    "    x = torch.clamp(x, 0, 200)\n",
    "    x = torch.log(x + 1e-6)\n",
    "    x = torch.clamp(x, -4.0, 0.5)\n",
    "    return x\n",
    "\n",
    "raw_refined_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(refined_height_transform)\n",
    "])\n",
    "\n",
    "class TerrainDataset(Dataset):\n",
    "    def __init__(self, folder, joint_transform, transform_seg, transform_height):\n",
    "        self.seg_files = sorted(glob.glob(os.path.join(folder, '*_i2.png')))\n",
    "        self.height_files = sorted(glob.glob(os.path.join(folder, '*_h.png')))\n",
    "        assert len(self.seg_files) == len(self.height_files)\n",
    "        self.joint_transform = joint_transform\n",
    "        self.transform_seg = transform_seg\n",
    "        self.transform_height = transform_height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seg_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seg = Image.open(self.seg_files[idx]).convert('RGB')\n",
    "        ht  = Image.open(self.height_files[idx]).convert('L')\n",
    "        seg, ht = self.joint_transform(seg, ht)\n",
    "        seg = self.transform_seg(seg)\n",
    "        ht  = self.transform_height(ht)\n",
    "        return seg, ht\n",
    "\n",
    "dataset_folder = r\"dataset\\_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40501bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height mean/std: -1.1637470722198486 1.797243595123291\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# 3. Compute Height Stats & Create Loader\n",
    "# ===================================================\n",
    "def compute_height_stats(ds, n=100):\n",
    "    loader = DataLoader(Subset(ds, list(range(min(len(ds), n)))), batch_size=8, shuffle=False)\n",
    "    vals = []\n",
    "    for _, h in loader:\n",
    "        vals.append(h.view(h.size(0), -1))\n",
    "    vals = torch.cat(vals, 0)\n",
    "    return vals.mean().item(), vals.std().item()\n",
    "\n",
    "temp_ds = TerrainDataset(dataset_folder, JointTransform(), transform_seg, raw_refined_transform)\n",
    "h_mean, h_std = compute_height_stats(temp_ds)\n",
    "print(\"Height mean/std:\", h_mean, h_std)\n",
    "\n",
    "final_height_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(refined_height_transform),\n",
    "    transforms.Normalize((h_mean,), (h_std,))\n",
    "])\n",
    "\n",
    "train_dataset = TerrainDataset(dataset_folder, JointTransform(), transform_seg, final_height_transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba43a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Check new height map distribution (for debugging).\n",
    "def check_height_distribution(dataset, num_samples=50):\n",
    "    loader = DataLoader(Subset(dataset, list(range(min(len(dataset), num_samples)))),\n",
    "                        batch_size=8, shuffle=False)\n",
    "    all_vals = []\n",
    "    for _, heights in loader:\n",
    "        all_vals.append(heights.view(heights.size(0), -1))\n",
    "    all_vals = torch.cat(all_vals, dim=0)\n",
    "    print(\"New Height Map Distribution:\")\n",
    "    print(\"Min:\", all_vals.min().item())\n",
    "    print(\"Max:\", all_vals.max().item())\n",
    "    print(\"Mean:\", all_vals.mean().item())\n",
    "    print(\"Std:\", all_vals.std().item())\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(all_vals.flatten().cpu().numpy(), bins=50, color='green', alpha=0.7)\n",
    "    plt.title(\"Normalized Height Map Distribution\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "check_height_distribution(train_dataset, num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 4. Model Definitions\n",
    "# ===================================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(c)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(c)\n",
    "        self.relu  = nn.ReLU(True)\n",
    "    def forward(self, x):\n",
    "        r = x\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return self.relu(x + r)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.InstanceNorm2d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.InstanceNorm2d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.InstanceNorm2d(1), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(True)\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g); x1 = self.W_x(x)\n",
    "        if g1.shape[2:] != x1.shape[2:]:\n",
    "            g1 = F.interpolate(g1, size=x1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        psi = self.relu(g1 + x1); psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class AttentionUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=1, f=64):\n",
    "        super().__init__()\n",
    "        self.down1 = self._down(in_ch, f, norm=False)\n",
    "        self.down2 = self._down(f, f*2)\n",
    "        self.down3 = self._down(f*2, f*4)\n",
    "        self.down4 = self._down(f*4, f*8)\n",
    "        self.bott = nn.Sequential(\n",
    "            nn.Conv2d(f*8, f*16, 3, padding=1), nn.InstanceNorm2d(f*16), nn.ReLU(True),\n",
    "            ResidualBlock(f*16), ResidualBlock(f*16)\n",
    "        )\n",
    "        self.up4 = nn.ConvTranspose2d(f*16, f*8, 3, stride=1, padding=1)\n",
    "        self.att4 = AttentionGate(F_g=f*8, F_l=f*8, F_int=f*4)\n",
    "        self.up3 = self._up(f*16, f*4)\n",
    "        self.att3 = AttentionGate(F_g=f*4, F_l=f*4, F_int=f*2)\n",
    "        self.up2 = self._up(f*8, f*2)\n",
    "        self.att2 = AttentionGate(F_g=f*2, F_l=f*2, F_int=f)\n",
    "        self.up1 = self._up(f*4, f*2)\n",
    "        self.up0 = self._up(f*3, f*2)\n",
    "        self.final = nn.Sequential(nn.Conv2d(f*2, out_ch, 1), nn.Hardtanh(-1, 1))\n",
    "\n",
    "    def _down(self, ic, oc, norm=True):\n",
    "        layers = [nn.Conv2d(ic, oc, 4, 2, 1)]\n",
    "        if norm:\n",
    "            layers.append(nn.InstanceNorm2d(oc))\n",
    "        layers.append(nn.LeakyReLU(0.2, True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _up(self, ic, oc):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(ic, oc, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(oc),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)                     \n",
    "        d2 = self.down2(d1)                    \n",
    "        d3 = self.down3(d2)                    \n",
    "        d4 = self.down4(d3)                    \n",
    "        b  = self.bott(d4)                     \n",
    "\n",
    "        u4 = self.up4(b)                       \n",
    "        d4a = self.att4(g=u4, x=d4)\n",
    "        if d4a.shape[2:] != u4.shape[2:]:\n",
    "            d4a = F.interpolate(d4a, size=u4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u4c = torch.cat([u4, d4a], dim=1)      \n",
    "\n",
    "        u3 = self.up3(u4c)                     \n",
    "        d3a = self.att3(g=u3, x=d3)\n",
    "        if d3a.shape[2:] != u3.shape[2:]:\n",
    "            d3a = F.interpolate(d3a, size=u3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u3c = torch.cat([u3, d3a], dim=1)      \n",
    "\n",
    "        u2 = self.up2(u3c)                     \n",
    "        d2a = self.att2(g=u2, x=d2)\n",
    "        if d2a.shape[2:] != u2.shape[2:]:\n",
    "            d2a = F.interpolate(d2a, size=u2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u2c = torch.cat([u2, d2a], dim=1)      \n",
    "\n",
    "        u1 = self.up1(u2c)                     \n",
    "        if d1.shape[2:] != u1.shape[2:]:\n",
    "            d1 = F.interpolate(d1, size=u1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u1c = torch.cat([u1, d1], dim=1)       \n",
    "\n",
    "        u0 = self.up0(u1c)                     \n",
    "        out = self.final(u0)                   \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb186956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=4, f=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_ch, f, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f, f*2, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(f*2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(f*2, f*4, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(f*4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f*4, f*8, 4, 1, 1),\n",
    "            nn.InstanceNorm2d(f*8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f*8, 1, 4, 1, 1)\n",
    "        )\n",
    "    def forward(self, seg, ht):\n",
    "        if seg.shape[2:] != ht.shape[2:]:\n",
    "            seg = F.interpolate(seg, size=ht.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([seg, ht], 1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 5. Losses & Penalty (Perceptual Loss Removed)\n",
    "# ===================================================\n",
    "# If you want to enable perceptual again, uncomment below:\n",
    "# try:\n",
    "#     global_vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features[:16].to(device).eval()\n",
    "# except:\n",
    "#     global_vgg = models.vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "# for p in global_vgg.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# def perceptual_loss(fake, real):\n",
    "#     if fake.size(1)==1: fake = fake.repeat(1,3,1,1)\n",
    "#     if real.size(1)==1: real = real.repeat(1,3,1,1)\n",
    "#     return F.l1_loss(global_vgg(fake), global_vgg(real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9537d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, seg, real, fake, epsilon=1e-6):\n",
    "    alpha = torch.rand(real.size(0),1,1,1, device=device)\n",
    "    inter = (alpha*real + (1-alpha)*fake).requires_grad_(True)\n",
    "    d_i = D(seg, inter)\n",
    "    grads = autograd.grad(outputs=d_i, inputs=inter,\n",
    "                          grad_outputs=torch.ones_like(d_i),\n",
    "                          create_graph=True, retain_graph=True)[0]\n",
    "    grads = grads.view(grads.size(0), -1)\n",
    "    norm = grads.norm(2,1) + epsilon\n",
    "    return ((norm-1)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211defc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 6. Init Models, Optimizers, Schedulers\n",
    "# ===================================================\n",
    "netG = AttentionUNetGenerator().to(device)\n",
    "netD = PatchGANDiscriminator().to(device)\n",
    "\n",
    "lr_G, lr_D = 2e-4, 1e-7\n",
    "optimizer_G = optim.Adam(netG.parameters(), lr=lr_G, betas=(0.5,0.999))\n",
    "optimizer_D = optim.Adam(netD.parameters(), lr=lr_D, betas=(0.5,0.999))\n",
    "\n",
    "scaler_G = torch.amp.GradScaler()\n",
    "scaler_D = torch.amp.GradScaler()\n",
    "\n",
    "lambda_L1 = 40\n",
    "lambda_adv_G = 1.0\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler_G = ReduceLROnPlateau(optimizer_G, mode='min', factor=0.5, patience=2)\n",
    "scheduler_D = ReduceLROnPlateau(optimizer_D, mode='min', factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e2738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CL502_14\\AppData\\Local\\Temp\\ipykernel_18028\\2073866226.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cp = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed at epoch 661\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# 7. Checkpoint Setup\n",
    "# ===================================================\n",
    "ckpt_dir = os.path.join(r\"C:\\Users\\CL502_14\\Desktop\\CV_GAN\",\"checkpoints\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "start_epoch = 1\n",
    "ckpt_path = os.path.join(ckpt_dir, \"checkpoint_latest.pth\")\n",
    "if os.path.exists(ckpt_path):\n",
    "    cp = torch.load(ckpt_path, map_location=device)\n",
    "    netG.load_state_dict(cp[\"netG_state_dict\"])\n",
    "    netD.load_state_dict(cp[\"netD_state_dict\"])\n",
    "    optimizer_G.load_state_dict(cp[\"optimizer_G_state_dict\"])\n",
    "    optimizer_D.load_state_dict(cp[\"optimizer_D_state_dict\"])\n",
    "    start_epoch = cp[\"epoch\"]+1\n",
    "    print(\"Resumed at epoch\", start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bab037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 661/1000: 100%|██████████| 1250/1250 [18:30<00:00,  1.13it/s, D_loss=0.6816, G_loss=8.0318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661 → Avg D: 1.2065, Avg G: 5.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 662/1000: 100%|██████████| 1250/1250 [18:19<00:00,  1.14it/s, D_loss=1.5215, G_loss=3.6856] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 662 → Avg D: 1.1871, Avg G: 5.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 663/1000: 100%|██████████| 1250/1250 [18:19<00:00,  1.14it/s, D_loss=0.5776, G_loss=7.9738] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 663 → Avg D: 1.1564, Avg G: 5.8357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 664/1000: 100%|██████████| 1250/1250 [18:35<00:00,  1.12it/s, D_loss=0.7266, G_loss=8.1774] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664 → Avg D: 1.1221, Avg G: 5.8971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 665/1000:  24%|██▍       | 299/1250 [04:27<14:09,  1.12it/s, D_loss=0.8613, G_loss=5.9942] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m lambda_adv_G \u001b[38;5;241m*\u001b[39m g_adv \u001b[38;5;241m+\u001b[39m lambda_L1 \u001b[38;5;241m*\u001b[39m l1\n\u001b[0;32m     44\u001b[0m scaler_G\u001b[38;5;241m.\u001b[39mscale(g_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 45\u001b[0m scaler_G\u001b[38;5;241m.\u001b[39mstep(optimizer_G); scaler_G\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     46\u001b[0m running_G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     47\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(D_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, G_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CL502_14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\CL502_14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\CL502_14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# 8. Training Loop (L1 + Adversarial only)\n",
    "# ===================================================\n",
    "num_epochs = 1000\n",
    "n_generator = 5\n",
    "hist = {\"G\":[], \"D\":[]}\n",
    "epoch = start_epoch\n",
    "\n",
    "while epoch <= num_epochs:\n",
    "    netG.train(); netD.train()\n",
    "    running_G, running_D = 0.0, 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=True)\n",
    "    \n",
    "    for seg, real_ht in loop:\n",
    "        seg, real_ht = seg.to(device), real_ht.to(device)\n",
    "        seg = F.interpolate(seg, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        real_ht = F.interpolate(real_ht, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # --- Discriminator ---\n",
    "        optimizer_D.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            fake_ht = netG(seg).detach()\n",
    "            real_pred = netD(seg, real_ht)\n",
    "            fake_pred = netD(seg, fake_ht)\n",
    "            loss_real = F.relu(1.0 - real_pred).mean()\n",
    "            loss_fake = F.relu(1.0 + fake_pred).mean()\n",
    "            # gp = compute_gradient_penalty(netD, seg, real_ht, fake_ht)\n",
    "            # d_loss = loss_real + loss_fake + lambda_gp * gp\n",
    "            d_loss = loss_real + loss_fake\n",
    "        scaler_D.scale(d_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm=1.0)\n",
    "        scaler_D.step(optimizer_D); scaler_D.update()\n",
    "        running_D += d_loss.item()\n",
    "\n",
    "        # --- Generator ---\n",
    "        for _ in range(n_generator):\n",
    "            optimizer_G.zero_grad()\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                fake_ht = netG(seg)\n",
    "                pred_fake = netD(seg, fake_ht)\n",
    "                g_adv = -pred_fake.mean()\n",
    "                l1 = F.l1_loss(fake_ht, real_ht)\n",
    "                g_loss = lambda_adv_G * g_adv + lambda_L1 * l1\n",
    "            scaler_G.scale(g_loss).backward()\n",
    "            scaler_G.step(optimizer_G); scaler_G.update()\n",
    "            running_G += g_loss.item()\n",
    "            loop.set_postfix(D_loss=f\"{d_loss.item():.4f}\", G_loss=f\"{g_loss.item():.4f}\")\n",
    "\n",
    "    avg_D = running_D / len(train_loader)\n",
    "    avg_G = running_G / (len(train_loader) * n_generator)\n",
    "    scheduler_D.step(avg_D)\n",
    "    scheduler_G.step(avg_G)\n",
    "    hist[\"D\"].append(avg_D)\n",
    "    hist[\"G\"].append(avg_G)\n",
    "    print(f\"Epoch {epoch} → Avg D: {avg_D:.4f}, Avg G: {avg_G:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 5 == 0:\n",
    "        cp = {\n",
    "            \"epoch\": epoch,\n",
    "            \"netG_state_dict\": netG.state_dict(),\n",
    "            \"netD_state_dict\": netD.state_dict(),\n",
    "            \"optimizer_G_state_dict\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D_state_dict\": optimizer_D.state_dict()\n",
    "        }\n",
    "        torch.save(cp, os.path.join(ckpt_dir, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "        torch.save(cp, ckpt_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch}\")\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(hist[\"G\"], label=\"G Loss\")\n",
    "    plt.plot(hist[\"D\"], label=\"D Loss\")\n",
    "    plt.legend(); plt.savefig(os.path.join(ckpt_dir,\"loss_curve.png\")); plt.close()\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 9. Save Final Models\n",
    "# ===================================================\n",
    "os.makedirs(\"final_checkpoints\", exist_ok=True)\n",
    "torch.save(netG.state_dict(), \"final_checkpoints/netG_final.pth\")\n",
    "torch.save(netD.state_dict(), \"final_checkpoints/netD_final.pth\")\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
