{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6044a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a1eaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 1. Device Setup\n",
    "# ===================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fa2306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 2. Data Preprocessing\n",
    "# ===================================================\n",
    "# A. Joint Transform for paired augmentation.\n",
    "class JointTransform:\n",
    "    def __init__(self, size=(512, 512)):\n",
    "        self.size = size\n",
    "    def __call__(self, seg, height):\n",
    "        seg = seg.resize(self.size, Image.BICUBIC)\n",
    "        height = height.resize(self.size, Image.BICUBIC)\n",
    "        # Random flips for augmentation\n",
    "        if random.random() > 0.5:\n",
    "            seg = seg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            height = height.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if random.random() > 0.5:\n",
    "            seg = seg.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            height = height.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return seg, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d06cf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Segmentation Transform: Convert to Tensor and Normalize to roughly [-1, 1].\n",
    "transform_seg = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e93ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Refined Height Transform Function.\n",
    "def refined_height_transform(x):\n",
    "    \"\"\"\n",
    "    Processes the height map (after ToTensor):\n",
    "      1. Clamp raw values to [0, 200]\n",
    "      2. Apply log transform\n",
    "      3. Clamp the log-transformed values to [-4.0, 0.5]\n",
    "    \"\"\"\n",
    "    x = torch.clamp(x, 0, 200)\n",
    "    x = torch.log(x + 1e-6)\n",
    "    x = torch.clamp(x, min=-4.0, max=0.5)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b685760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Create a raw transform (without normalization) for computing statistics.\n",
    "raw_refined_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: refined_height_transform(x))\n",
    "])\n",
    "# Temporarily, use the raw transform.\n",
    "transform_height = raw_refined_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "401e4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These normalization parameters must match those computed during training.\n",
    "height_mean = -1.1637470722198486\n",
    "height_std  = 1.797243595123291\n",
    "\n",
    "final_height_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: refined_height_transform(x)),\n",
    "    transforms.Normalize((height_mean,), (height_std,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "529ef675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 3. Dataset Definition\n",
    "# ===================================================\n",
    "class TerrainDataset(Dataset):\n",
    "    def __init__(self, folder, joint_transform=None, transform_seg=None, transform_height=None):\n",
    "        self.folder = folder\n",
    "        self.seg_files = sorted(glob.glob(os.path.join(folder, '*_i2.png')))\n",
    "        self.height_files = sorted(glob.glob(os.path.join(folder, '*_h.png')))\n",
    "        assert len(self.seg_files) == len(self.height_files), \"Mismatch between segmentation and height images.\"\n",
    "        self.joint_transform = joint_transform\n",
    "        self.transform_seg = transform_seg\n",
    "        self.transform_height = transform_height\n",
    "    def __len__(self):\n",
    "        return len(self.seg_files)\n",
    "    def __getitem__(self, idx):\n",
    "        seg_img = Image.open(self.seg_files[idx]).convert('RGB')\n",
    "        height_img = Image.open(self.height_files[idx]).convert('L')\n",
    "        if self.joint_transform:\n",
    "            seg_img, height_img = self.joint_transform(seg_img, height_img)\n",
    "        if self.transform_seg:\n",
    "            seg_img = self.transform_seg(seg_img)\n",
    "        if self.transform_height:\n",
    "            height_img = self.transform_height(height_img)\n",
    "        return seg_img, height_img\n",
    "\n",
    "# Use the same dataset folder as in train.py.\n",
    "dataset_folder = r\"dataset\\_dataset\"\n",
    "val_dataset = TerrainDataset(\n",
    "    folder=dataset_folder,\n",
    "    joint_transform=JointTransform(),\n",
    "    transform_seg=transform_seg,\n",
    "    transform_height=final_height_transform\n",
    ")\n",
    "# Use a moderate batch size for validation.\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31cf3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 4. Model Definitions\n",
    "# ===================================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(c)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(c)\n",
    "        self.relu  = nn.ReLU(True)\n",
    "    def forward(self, x):\n",
    "        r = x\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return self.relu(x + r)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.InstanceNorm2d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.InstanceNorm2d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.InstanceNorm2d(1), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(True)\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g); x1 = self.W_x(x)\n",
    "        if g1.shape[2:] != x1.shape[2:]:\n",
    "            g1 = F.interpolate(g1, size=x1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        psi = self.relu(g1 + x1); psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class AttentionUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=1, f=64):\n",
    "        super().__init__()\n",
    "        self.down1 = self._down(in_ch, f, norm=False)\n",
    "        self.down2 = self._down(f, f*2)\n",
    "        self.down3 = self._down(f*2, f*4)\n",
    "        self.down4 = self._down(f*4, f*8)\n",
    "        self.bott = nn.Sequential(\n",
    "            nn.Conv2d(f*8, f*16, 3, padding=1), nn.InstanceNorm2d(f*16), nn.ReLU(True),\n",
    "            ResidualBlock(f*16), ResidualBlock(f*16)\n",
    "        )\n",
    "        self.up4 = nn.ConvTranspose2d(f*16, f*8, 3, stride=1, padding=1)\n",
    "        self.att4 = AttentionGate(F_g=f*8, F_l=f*8, F_int=f*4)\n",
    "        self.up3 = self._up(f*16, f*4)\n",
    "        self.att3 = AttentionGate(F_g=f*4, F_l=f*4, F_int=f*2)\n",
    "        self.up2 = self._up(f*8, f*2)\n",
    "        self.att2 = AttentionGate(F_g=f*2, F_l=f*2, F_int=f)\n",
    "        self.up1 = self._up(f*4, f*2)\n",
    "        self.up0 = self._up(f*3, f*2)\n",
    "        self.final = nn.Sequential(nn.Conv2d(f*2, out_ch, 1), nn.Hardtanh(-1, 1))\n",
    "\n",
    "    def _down(self, ic, oc, norm=True):\n",
    "        layers = [nn.Conv2d(ic, oc, 4, 2, 1)]\n",
    "        if norm:\n",
    "            layers.append(nn.InstanceNorm2d(oc))\n",
    "        layers.append(nn.LeakyReLU(0.2, True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _up(self, ic, oc):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(ic, oc, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(oc),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)                     \n",
    "        d2 = self.down2(d1)                    \n",
    "        d3 = self.down3(d2)                    \n",
    "        d4 = self.down4(d3)                    \n",
    "        b  = self.bott(d4)                     \n",
    "\n",
    "        u4 = self.up4(b)                       \n",
    "        d4a = self.att4(g=u4, x=d4)\n",
    "        if d4a.shape[2:] != u4.shape[2:]:\n",
    "            d4a = F.interpolate(d4a, size=u4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u4c = torch.cat([u4, d4a], dim=1)      \n",
    "\n",
    "        u3 = self.up3(u4c)                     \n",
    "        d3a = self.att3(g=u3, x=d3)\n",
    "        if d3a.shape[2:] != u3.shape[2:]:\n",
    "            d3a = F.interpolate(d3a, size=u3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u3c = torch.cat([u3, d3a], dim=1)      \n",
    "\n",
    "        u2 = self.up2(u3c)                     \n",
    "        d2a = self.att2(g=u2, x=d2)\n",
    "        if d2a.shape[2:] != u2.shape[2:]:\n",
    "            d2a = F.interpolate(d2a, size=u2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u2c = torch.cat([u2, d2a], dim=1)      \n",
    "\n",
    "        u1 = self.up1(u2c)                     \n",
    "        if d1.shape[2:] != u1.shape[2:]:\n",
    "            d1 = F.interpolate(d1, size=u1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u1c = torch.cat([u1, d1], dim=1)       \n",
    "\n",
    "        u0 = self.up0(u1c)                     \n",
    "        out = self.final(u0)                   \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a00b2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=4, f=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_ch, f, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f, f*2, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(f*2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(f*2, f*4, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(f*4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f*4, f*8, 4, 1, 1),\n",
    "            nn.InstanceNorm2d(f*8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(f*8, 1, 4, 1, 1)\n",
    "        )\n",
    "    def forward(self, seg, ht):\n",
    "        if seg.shape[2:] != ht.shape[2:]:\n",
    "            seg = F.interpolate(seg, size=ht.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([seg, ht], 1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f644345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===================================================\n",
    "# # 5. Global VGG Model for Perceptual Loss (loaded once)\n",
    "# # ===================================================\n",
    "# try:\n",
    "#     global_vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features[:16].to(device).eval()\n",
    "# except Exception:\n",
    "#     global_vgg = models.vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "# for param in global_vgg.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# def perceptual_loss(fake, real):\n",
    "#     if fake.shape[1] == 1:\n",
    "#         fake = fake.repeat(1, 3, 1, 1)\n",
    "#     if real.shape[1] == 1:\n",
    "#         real = real.repeat(1, 3, 1, 1)\n",
    "#     f_fake = global_vgg(fake)\n",
    "#     f_real = global_vgg(real)\n",
    "#     return nn.functional.l1_loss(f_fake, f_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6a3de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, seg, real_samples, fake_samples, epsilon=1e-6):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = D(seg, interpolates)\n",
    "    fake = torch.ones(d_interpolates.size(), device=device)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    grad_norm = gradients.norm(2, dim=1) + epsilon\n",
    "    grad_norm = torch.clamp(grad_norm, max=1e3)\n",
    "    gradient_penalty = ((grad_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ad30061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CL502_14\\AppData\\Local\\Temp\\ipykernel_12936\\1740178413.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(os.path.join(\"checkpoints\",\"checkpoint_latest.pth\"), map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 4) Instantiate & Load Checkpoint\n",
    "# ================================\n",
    "netG = AttentionUNetGenerator().to(device).eval()\n",
    "netD = PatchGANDiscriminator().to(device).eval()\n",
    "\n",
    "ckpt = torch.load(os.path.join(\"checkpoints\",\"checkpoint_latest.pth\"), map_location=device)\n",
    "netG.load_state_dict(ckpt[\"netG_state_dict\"])\n",
    "netD.load_state_dict(ckpt[\"netD_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f215d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_L1 = 50\n",
    "lambda_adv_G = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28142945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint for validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CL502_14\\AppData\\Local\\Temp\\ipykernel_12936\\3191433674.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed at epoch 541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PatchGANDiscriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================\n",
    "# 7. Load Checkpoint for Validation\n",
    "# ===================================================\n",
    "ckpt_dir = r\"checkpoints\"\n",
    "checkpoint_path = os.path.join(ckpt_dir, \"checkpoint_latest.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint for validation...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    # Instantiate models first\n",
    "    netG = AttentionUNetGenerator(in_ch=3, out_ch=1, f=64).to(device)\n",
    "    netD = PatchGANDiscriminator(in_ch=4, f=64).to(device)\n",
    "    netG.load_state_dict(checkpoint[\"netG_state_dict\"])\n",
    "    netD.load_state_dict(checkpoint[\"netD_state_dict\"])\n",
    "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
    "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Resumed at epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Checkpoint not found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "netG.eval()\n",
    "netD.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e44b962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1250/1250 [06:37<00:00,  3.14it/s, D_loss=0.6549, G_loss=9.4122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation → Avg D Loss: 0.6210, Avg G Loss: 9.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5) Validation Loop\n",
    "# ================================\n",
    "running_D, running_G = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(val_loader, desc=\"Validating\", leave=True)\n",
    "    for seg, real_ht in loop:\n",
    "        seg, real_ht = seg.to(device), real_ht.to(device)\n",
    "\n",
    "        # Discriminator (hinge GAN)\n",
    "        fake_ht = netG(seg)\n",
    "        real_pred = netD(seg, real_ht)\n",
    "        fake_pred = netD(seg, fake_ht.detach())\n",
    "        loss_real = F.relu(1.0 - real_pred).mean()\n",
    "        loss_fake = F.relu(1.0 + fake_pred).mean()\n",
    "        d_loss = loss_real + loss_fake\n",
    "\n",
    "        # Generator (adv + L1)\n",
    "        g_adv = -netD(seg, fake_ht).mean()\n",
    "        l1    = F.l1_loss(fake_ht, real_ht)\n",
    "        g_loss = lambda_adv_G * g_adv + lambda_L1 * l1\n",
    "\n",
    "        running_D += d_loss.item()\n",
    "        running_G += g_loss.item()\n",
    "        loop.set_postfix(D_loss=f\"{d_loss:.4f}\", G_loss=f\"{g_loss:.4f}\")\n",
    "\n",
    "avg_D = running_D / len(val_loader)\n",
    "avg_G = running_G / len(val_loader)\n",
    "print(f\"\\nValidation → Avg D Loss: {avg_D:.4f}, Avg G Loss: {avg_G:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e174efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
